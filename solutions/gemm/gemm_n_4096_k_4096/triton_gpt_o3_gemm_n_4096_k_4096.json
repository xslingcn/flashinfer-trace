{
    "name": "triton_gpt_o3_gemm_n_4096_k_4096",
    "definition": "gemm_n_4096_k_4096",
    "description": "High-performance Triton kernel for C = A @ B.T with dynamic M and fixed N=K=4096 (captured from Llama self-attn.o_proj). Auto-tunes over several block shapes, uses float32 accumulation, and targets NVIDIA Hopper/Blackwell GPUs.",
    "author": "gpt-o3",
    "spec": {
      "language": "Triton",
      "target_hardware": [
        "NVIDIA_H100",
        "NVIDIA_B100",
        "NVIDIA_B200"
      ],
      "dependencies": [
        "torch",
        "triton >= 2.4.0"
      ],
      "entry_point": "main.py::run",
      "build_commands": []
    },
    "sources": [
      {
        "path": "main.py",
        "content": "import torch\nimport triton\nimport triton.language as tl\n\n# -------------------------------------------------------------------------------------\n#  Triton GEMM kernel:  C[M, N] = A[M, K] @ B[N, K]^T   with M dynamic, N = K = 4096\n# -------------------------------------------------------------------------------------\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32,  \"GROUP_M\": 8},  num_stages=4, num_warps=8),\n        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 256, \"BLOCK_K\": 32,  \"GROUP_M\": 4},  num_stages=4, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64,  \"BLOCK_K\": 32,  \"GROUP_M\": 4},  num_stages=4, num_warps=8),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 64,  \"GROUP_M\": 8},  num_stages=3, num_warps=8)\n    ],\n    key=[\"M\"]\n)\n@triton.jit\ndef _gemm_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    N: tl.constexpr, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr):\n    \"\"\"\n    Computes a BLOCK_M x BLOCK_N tile of C.  B is accessed in row-major order and\n    logically transposed to align with tl.dot((M,K),(K,N)).\n    \"\"\"\n    # program id\n    pid = tl.program_id(axis=0)\n\n    # --- decompose pid into two-dim tile indices, with grouping on M to improve L2 hit rate ---\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)  # compile-time constant (N = 4096)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = tl.minimum(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)               # [BLOCK_M]\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)               # [BLOCK_N]\n    offs_k  = tl.arange(0, BLOCK_K)                                 # [BLOCK_K]\n\n    # pointers\n    a_ptrs = A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_M, BLOCK_K)\n    b_ptrs = B_ptr + (offs_k[:, None]  * stride_bk + offs_bn[None, :] * stride_bn) # (BLOCK_K, BLOCK_N)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k0 in range(0, K, BLOCK_K):  # K is constexpr = 4096 => compile-time unrolled\n        a = tl.load(a_ptrs, mask=offs_am[:, None] < M)\n        b = tl.load(b_ptrs, mask=offs_bn[None, :] < N)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    # write-back\n    c = acc.to(tl.float16)\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs  = C_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\n# -------------------------------------------------------------------------\n#  Python launcher\n# -------------------------------------------------------------------------\n\ndef run(A: torch.Tensor, B: torch.Tensor):\n    \"\"\"Launches the Triton kernel.\n\n    Args:\n        A: [M, 4096] float16 contiguous.\n        B: [4096, 4096] float16 contiguous.\n    Returns:\n        dict with key 'C' containing [M, 4096] float16.\n    \"\"\"\n    assert A.dtype == torch.float16 and B.dtype == torch.float16, \"Input dtypes must be fp16\"\n    M, K_ = A.shape\n    N_, Kb = B.shape\n    assert K_ == 4096 and N_ == 4096 and Kb == 4096, \"Definition expects N = K = 4096\"\n\n    C = torch.empty((M, 4096), device=A.device, dtype=A.dtype)\n\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(4096, META[\"BLOCK_N\"]),\n    )\n\n    _gemm_kernel[grid](\n        A, B, C,\n        M,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        4096, 4096  # N, K as constexpr\n    )\n    return {\"C\": C}\n"
      }
    ]
  }
  