{
  "name": "mla_paged_prefill_h128_ckv512_kpe64_ps1",
  "description": "Multi-head Latent Attention (MLA) paged incremental prefill with Matrix Absorption. Used for incremental prefill in DeepSeek-V2/V3 models.",
  "type": "mla",
  "tags": [
    "stage:prefill",
    "status:draft",
    "model:deepseek-v2",
    "model:deepseek-v3",
    "model:deepseek-r1"
  ],
  "axes": {
    "batch_size": {
      "type": "var"
    },
    "num_qo_heads": {
      "type": "const",
      "value": 128
    },
    "head_dim_ckv": {
      "type": "const",
      "value": 512
    },
    "head_dim_kpe": {
      "type": "const",
      "value": 64
    },
    "page_size": {
      "type": "const",
      "value": 1
    },
    "total_q": {
      "type": "var"
    },
    "num_pages": {
      "type": "var"
    },
    "qo_len_indptr": {
      "type": "var"
    },
    "kv_len_indptr": {
      "type": "var"
    }
  },
  "constraints": [
    "qo_len_indptr == batch_size + 1",
    "kv_len_indptr == batch_size + 1",
    "total_q == qo_indptr[-1].item()"
  ],
  "inputs": {
    "q_nope": {
      "shape": [
        "total_q",
        "num_qo_heads",
        "head_dim_ckv"
      ],
      "dtype": "bfloat16"
    },
    "q_pe": {
      "shape": [
        "total_q",
        "num_qo_heads",
        "head_dim_kpe"
      ],
      "dtype": "bfloat16"
    },
    "ckv_cache": {
      "shape": [
        "num_pages",
        "page_size",
        "head_dim_ckv"
      ],
      "dtype": "bfloat16"
    },
    "kpe_cache": {
      "shape": [
        "num_pages",
        "page_size",
        "head_dim_kpe"
      ],
      "dtype": "bfloat16"
    },
    "qo_indptr": {
      "shape": [
        "qo_len_indptr"
      ],
      "dtype": "int32"
    },
    "kv_indptr": {
      "shape": [
        "kv_len_indptr"
      ],
      "dtype": "int32"
    },
    "kv_indices": {
      "shape": [
        "num_pages"
      ],
      "dtype": "int32"
    },
    "kv_len_arr": {
      "shape": [
        "batch_size"
      ],
      "dtype": "int32"
    },
    "sm_scale": {
      "shape": [],
      "dtype": "float32"
    }
  },
  "outputs": {
    "output": {
      "shape": [
        "total_q",
        "num_qo_heads",
        "head_dim_ckv"
      ],
      "dtype": "bfloat16"
    },
    "lse": {
      "shape": [
        "total_q",
        "num_qo_heads"
      ],
      "dtype": "float32"
    }
  },
  "reference": "import torch\nimport math\n\n\n@torch.no_grad()\ndef run(q_nope, q_pe, ckv_cache, kpe_cache, qo_indptr, kv_indptr, kv_indices, kv_len_arr, sm_scale):\n    total_q, num_qo_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n    num_pages = ckv_cache.shape[0]\n    page_size = ckv_cache.shape[1]\n    batch_size = kv_len_arr.shape[0]\n    \n    # Check constants\n    assert num_qo_heads == 128\n    assert head_dim_ckv == 512\n    assert head_dim_kpe == 64\n    assert page_size == 1\n    \n    # Check constraints\n    assert qo_indptr.shape[0] == batch_size + 1\n    assert kv_indptr.shape[0] == batch_size + 1\n    \n    device = q_nope.device\n    \n    # Flatten KV caches since page_size=1\n    Kc_all = ckv_cache.squeeze(1).to(torch.float32)  # [num_pages, head_dim_ckv]\n    Kp_all = kpe_cache.squeeze(1).to(torch.float32)  # [num_pages, head_dim_kpe]\n    \n    output = torch.zeros(\n        (total_q, num_qo_heads, head_dim_ckv), dtype=torch.bfloat16, device=device\n    )\n    lse = torch.full(\n        (total_q, num_qo_heads), -float(\"inf\"), dtype=torch.float32, device=device\n    )\n    \n    for b in range(batch_size):\n        # Get query range for this batch element\n        q_start = int(qo_indptr[b].item())\n        q_end = int(qo_indptr[b + 1].item())\n        \n        # Get KV page range for this batch element\n        page_beg = int(kv_indptr[b].item())\n        page_end = int(kv_indptr[b + 1].item())\n        \n        if q_start >= q_end or page_beg >= page_end:\n            # No queries or KV for this batch element\n            continue\n        \n        # Get actual KV length for this batch\n        L_tokens = int(kv_len_arr[b].item())\n        \n        if L_tokens <= 0:\n            continue\n        \n        # Get page indices for this batch\n        pages = kv_indices[page_beg:page_end]\n        \n        # Since page_size=1, pages are token indices\n        # We only need the first L_tokens\n        tok_idx = pages[:L_tokens].to(torch.long)\n        \n        # Extract KV for this batch\n        Kc = Kc_all[tok_idx]  # [L_tokens, head_dim_ckv]\n        Kp = Kp_all[tok_idx]  # [L_tokens, head_dim_kpe]\n        \n        # Extract queries for this batch\n        q_nope_batch = q_nope[q_start:q_end].to(torch.float32)  # [q_len, num_heads, head_dim_ckv]\n        q_pe_batch = q_pe[q_start:q_end].to(torch.float32)  # [q_len, num_heads, head_dim_kpe]\n        \n        q_len = q_end - q_start\n        \n        # Compute attention scores for each query\n        for i in range(q_len):\n            qn = q_nope_batch[i]  # [num_heads, head_dim_ckv]\n            qp = q_pe_batch[i]  # [num_heads, head_dim_kpe]\n            \n            # Compute logits with MLA decomposition\n            logits = (qn @ Kc.T) + (qp @ Kp.T)  # [num_heads, L_tokens]\n            logits_scaled = logits * sm_scale\n            \n            # Apply causal mask for prefill\n            # Query position in the full sequence\n            query_pos = q_start + i\n            for j in range(L_tokens):\n                # Key position in the sequence\n                key_pos = j\n                if query_pos < key_pos:\n                    logits_scaled[:, j] = -float(\"inf\")\n            \n            # Compute 2-base LSE\n            lse[q_start + i] = torch.logsumexp(logits_scaled, dim=-1) / math.log(2.0)\n            \n            # Apply softmax and compute output\n            attn = torch.softmax(logits_scaled, dim=-1)  # [num_heads, L_tokens]\n            out = attn @ Kc  # [num_heads, head_dim_ckv]\n            output[q_start + i] = out.to(torch.bfloat16)\n    \n    return {\"output\": output, \"lse\": lse}"
}